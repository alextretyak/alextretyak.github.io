Н‘‘Выбор кодировки для компилятора (ссылка на статью)’[http://compiler.su/vybor-kodirovki-dlya-kompilyatora.php]’

Комментарии:
---
*‘2021-01-17’ ~‘alextretyak’

> Если в язык добавляем поддержку кириллицы, то разбор уже такой:
Тут наверное ошибка — имеется в виду «добавляем поддержку русского алфавита», а не кириллицы[https://ru.wikipedia.org/wiki/Кириллица].
[Причём «русского алфавита без буквы ё» :)(:.]
[[[[
(==      &#124;&#124; ( 'A' <= символ && символ <= 'Я' &#124;&#124; 'a' <= символ && символ <= 'я')==)
А как же буква «ё»?
Во всех известных мне кодировках, поддерживающих русский язык (1251, 866, KOI8-R и даже в Юникоде), буква «ё» находится за пределами диапазона от «а» до «я», поэтому необходимо добавить отдельную проверку на «ё» и «Ё».
]]]]
>‘#(C)‘
    if (! (массив признаков [символ] && ПЕРВЫЙ СИМВОЛ ИДЕНТИФИКАТОРА ))
    ...
        if (! (массив признаков [символ] && СЛЕДУЮЩИЙ СИМВОЛ ИДЕНТИФИКАТОРА ))
’’
Как я предполагаю ~‘ПЕРВЫЙ СИМВОЛ ИДЕНТИФИКАТОРА’ это константа 1, а ~‘СЛЕДУЮЩИЙ СИМВОЛ ИДЕНТИФИКАТОРА’ — это константа 2.
Но в таком случае следует использовать поразрядное И (&), а не логическое (&&).
[[[[
Статья слишком теоретическая.
Попробую восполнить этот недостаток посредством данного комментария.]]]][[[[
Теперь пара замечаний с точки зрения практического использования. Признаки может быть выгоднее хранить не в одном массиве, а в разных — по одному массиву из одноразрядных значений (std::vector<bool>) на каждый признак.
]]]]

Признаю целесообразность ограничиться только поддержкой базовой плоскости Юникода (первые 65536 символов), тогда реализация в виде простого массива из 65536 элементов почти оптимальна. Но если поразмышлять на этот счёт...
Если посмотреть в ‘документацию MSVC’[https://docs.microsoft.com/en-us/cpp/cpp/identifiers-cpp?view=msvc-140], то там приводятся конкретные диапазоны символов Юникода, которые можно использовать в идентификаторах. Наибольший код допустимого в идентификаторах символа составляет EFFFD, что в десятичной системе — 983037. Таким образом, потребуется массив из 983038 элементов. Но. А что, если признаки символов хранить не в одном массиве, а в разных — по одному массиву на каждый признак. И если вместо массива использовать hash-set\хэш-множество. Я [[[[не поленился и ]]]]написал простенькую программу[https://www.dropbox.com/s/ixpifmqdu818in2/MSVC_Identifiers.py?dl=0], которая суммирует все диапазоны из документации MSVC и, таким образом, считает сколько символов допустимо использовать в идентификаторах в MSVC. Получилось 971380. Теперь, вместо того, чтобы хранить символы, которые допустимо использовать в идентификаторах, можно хранить только недопустимые, ведь их всего лишь 11658 (983038 - 971380)!

P.S. Вопрос автору сайта: а почему символ «'‘&#124;’'» [даже внутри блока кода] заменяется на «!» (восклицательный знак) при нажатии на «Предварительный просмотр»?
---
*‘2021-01-18’ ~‘alextretyak’

Дабы закрыть эту тему решил провести небольшое исследование.

Как я уже писал в[[[ [-своём-]]]] предыдущем сообщении, в Microsoft Visual C++ используется определённый набор диапазонов кодов символов, которые разрешено использовать в идентификаторах. Оказывается, эти диапазоны прописаны в стандарте C++ (в Annex E[[[https://stackoverflow.com/questions/50003386/does-c17-allow-a-non-ascii-character-as-an-identifier <- google:‘c++ non acsii identifiers’]]]), и они также используются например в языке Swift[https://docs.swift.org/swift-book/ReferenceManual/LexicalStructure.html#ID412].
[[[[Причём, что удивительно, эти наборы совпадают! ]]]]Однако, откуда взялись эти диапазоны[[[, впрочем,]]] остаётся загадкой.

В языке C# уже лучше. В том смысле, что в его спецификации[https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/language-specification/lexical-structure#identifiers] не зашиты непосредственно коды символов, а указаны категории символов Юникода (а именно: ‘Lu’, ‘Ll’, ‘Lt’, ‘Lm’, ‘Lo’ и ‘Nl’), которые можно использовать в идентификаторах. (Категории всех символов Юникода указаны в файле UnicodeData.txt[https://www.unicode.org/Public/13.0.0/ucd/UnicodeData.txt][[[На эту ссылку я вышел путём дебаггинга файла makeunicodedata.py[https://github.com/python/cpython/blob/master/Tools/unicode/makeunicodedata.py <- https://github.com/python/cpython/blob/master/Modules/unicodedata.c <- google:‘unicodedata category github’]]]] в 3-й колонке.)
Аналогично, в языке Go используются[https://golang.org/ref/spec#unicode_letter] категории ‘Lu’, ‘Ll’, ‘Lt’, ‘Lm’ и ‘Lo’.

‘В Python’[https://www.python.org/dev/peps/pep-3131/] ещё лучше — всё определение идентификатора помещается в одной строчке:
#‘
The identifier syntax is <XID_Start> <XID_Continue>*
’

Где #‘<XID_Start>’ и #‘<XID_Continue>’ — категории символов для идентификаторов, которые [категории] определены прямо в стандарте Unicode! {Конкретно: в файле DerivedCoreProperties.txt[https://www.unicode.org/Public/13.0.0/ucd/DerivedCoreProperties.txt][[[Ссылка получена из ссылки [https://www.python.org/dev/peps/pep-3131/]:‘[6] http://www.unicode.org/Public/4.1.0/ucd/DerivedCoreProperties.txt’]]]}
[В JavaScript аналогично используются[https://www.262.ecma-international.org/11.0/index.html#prod-IdentifierName] категории #‘ID_Start’ и #‘ID_Continue’ {а до версии ECMAScript 2015 использовались[https://www.262.ecma-international.org/5.1/index.html#sec-7.6] категории такие же как в C#}.]

Но, как говорил В.: «Мы пойдём другим путём!»
Ну не хочется мне включать коды символов[[[ [диапазоны, категории]]]] или их диапазоны в исходный текст лексического анализатора без крайней [[[надобности/]]]необходимости. И я решил разобраться с теми средствами, которые предоставляют языки Python и C++ [на которых написан или в которые транслируется лексический анализатор языка 11l].

В Python есть метод `isalpha()`, который возвращает `True` для всех символов, принадлежащих одной из категорий ‘Lm’, ‘Lt’, ‘Lu’, ‘Ll’ или ‘Lo’. Что почти в точности совпадает с разрешёнными символами в C# (не считая редко используемую категорию ‘Nl’) и совпадает с разрешёнными символами в Go. Поэтому в Python-коде анализатора можно спокойно использовать `isalpha()`[[[ и не париться]]]. [Ещё в Python есть встроенный модуль `unicodedata`, функция `unicodedata.category()` которого возвращает категорию заданного символа, но считаю, что можно обойтись `isalpha()`.]

В C/C++ есть функция `iswalpha()`, которая возвращает «не пойми что», причём ещё и в зависимости от текущей локали.
В попытке разобраться с этой функцией я составил такую табличку:
#‘
                                    ┌──────────┬────────┐
                                    │ макс.код │ кол-во │
┌───────────────────────────────────┼──────────┼────────┤
│ MSVC identifiers                  │   983037 │ 971380 │
│ isalpha() in Python               │   195101 │ 125419 │
│ Unicode 13.0.0 Letter             │   195101 │ 125419 │
│ iswalpha() in MSVC (any locale)   │  1114076 │ 811189 │
│ iswalpha() in GCC (no setlocale)  │      122 │     52 │
│ iswalpha() in GCC (setlocale "C") │      122 │     52 │
│ iswalpha() in GCC (setlocale "")  │   195101 │  94318 │
└───────────────────────────────────┴──────────┴────────┘
’
(Исходный код ‘для вычисления’/получения всех этих циферок доступен тут[https://www.dropbox.com/sh/qo3rc2dw5c2pzx8/AACUTsoN-IPIzjPbEn_zBZipa?dl=0].)

Какой вывод/итог?...
...А решайте сами. :)(:
Я пока остановился на варианте «`isalpha()` для Python», «`iswalpha()` [с предварительным вызовом `setlocale(LC_CTYPE, "");`] для C++». Теперь лексические анализаторы транспайлеров Python → 11l и 11l → C++ поддерживают кириллицу в идентификаторах, а это собственно то, ради чего [[[этот вопрос и поднимался]]]это исследование и проводилось.
